---
title: "Women in Geo Legacy Project"
author: "Netra Bhandari, Stephanie Wegscheider"
format: html
editor: visual
---

## Legacy project (not yet complete)

Comparing R and Python for a data analysis workflow using geospatial data and benchmark the performances of the different spatial packages.

## Why benchmarking in R and Python??

Benchmarking in R and Python is crucial due to their extensive use in data science, statistics, and machine learning. Both languages have distinct characteristics and their own sets of tools and libraries for performance measurement. Here's why benchmarking is particularly important and commonly performed in both R and Python:

### **1. Performance Optimization**

Both R and Python are interpreted languages, which can sometimes perform slower compared to compiled languages like C or Java. In fields where processing large datasets or running complex algorithms is common, efficient code can significantly reduce execution time and resource consumption. Benchmarking helps identify slow sections of code and opportunities for optimization.

### **2. Algorithm Comparison**

Data scientists and researchers often use multiple approaches to solve a problem. Benchmarking allows them to compare the performance of different algorithms or models in terms of speed, accuracy, and resource usage. This comparison is essential for choosing the most appropriate method for specific data or computational constraints.

### **3. Library and Package Evaluation**

Both R and Python have vibrant communities that contribute libraries and packages. Developers and users frequently benchmark these to ensure they are using the most efficient tools available. For example, in Python, one might compare data manipulation operations in Pandas vs. Dask to determine the best tool for handling large datasets.

### **4. Scalability Testing**

Benchmarking tests how code performs as the size of the dataset grows. This scalability testing is crucial for applications expected to handle varying amounts of data over time, ensuring that the software is robust and performs well under different load conditions.

### **5. Teaching and Learning**

For educators and learners, benchmarking provides practical insights into how different coding practices affect performance. It can be an excellent teaching tool to illustrate the importance of efficient coding and selecting the right tool for the job.

### **6. Reproducibility**

In scientific research, ensuring that computational experiments are reproducible is essential. Benchmarking can provide a measure of how software performance might vary across different hardware or software environments, which is crucial for replicating study results.

### **Tools for Benchmarking**

-   **In R**, packages like **`microbenchmark`** and **`bench`** offer detailed profiling for small pieces of code, focusing on execution time and memory usage.

-   **In Python**, tools like **`timeit`** for small code snippets and **`profile`** or **`cProfile`** for larger applications help measure the execution time and identify performance bottlenecks.

In both languages, the integration of these tools into the development and research workflow promotes the creation of more efficient, robust, and reliable code, which is essential in professional and academic settings.

## Questions

-   How is GDP and life expectancy related across world?

-   Male vs female life expectancy

## Anlaysis in R

1.  Load in your needed libraries.

    We will use two very popular geospatial libraries - *terra* and *raster* and compare their performances.

```{r, message=FALSE, warning=FALSE}

if (!require(pacman)) install.packages("pacman")
library(pacman)
#load libraries
pacman::p_load(terra,raster, reticulate, tidyverse, dplyr, skimr, janitor)

```

2.  Read in your desired dataset. Here we are looking at the gross domestic product and the life expectancy in world.

```{r}

#load in the data

gdp = read.csv("./gdp-per-capita-penn-world-table.csv")
life_expectancy = read.csv("./life-expectancy.csv")

```

Let's have an overview of our dataset

```{r}
skim(gdp)
skim(life_expectancy)
```

``` R


```

```         
```

```{r}
# Clean the column names
gdp <- clean_names(gdp)
life_expectancy <- clean_names(life_expectancy)
merged_data <- merge(gdp, life_expectancy, by = c("entity", "year","code"))

# Correct the country names
name_correction <- c(
  "Antigua and Barbuda" = "Antigua & Barbuda",
  "Bosnia and Herzegovina" = "Bosnia & Herzegovina",
  "Brunei" = "Brunei Darussalam",
  "Cote d'Ivoire" = "CÃ´te d'Ivoire",
  "Czechia" = "Czech Republic",
  "Democratic Republic of Congo" = "Democratic Republic of the Congo",
  "Eswatini" = "Swaziland",
  "Iran" = "Iran (Islamic Republic of)",
  "Laos" = "Lao People's Democratic Republic",
  "Moldova" = "Moldova, Republic of",
  "North Macedonia" = "The former Yugoslav Republic of Macedonia",
  "Palestine" = "West Bank",
  "Russia" = "Russian Federation",
  "Sint Maarten (Dutch part)" = "Netherlands Antilles",
  "South Korea" = "Republic of Korea",
  "Syria" = "Syrian Arab Republic",
  "Tanzania" = "United Republic of Tanzania",
  "United Kingdom" = "U.K. of Great Britain and Northern Ireland",
  "United States" = "United States of America"
)


# Apply the corrections to both datasets
gdp$entity <- recode(gdp$entity, !!!name_correction)
life_expectancy$entity <- recode(life_expectancy$entity, !!!name_correction)

merged_data <- merged_data %>% rename(life_expectancy = period_life_expectancy_at_birth_sex_all_age_0,
                                      gdp = gdp_per_capita_output_multiple_price_benchmarks)

# Data cleaning
merged_data <- na.omit(merged_data)

correlation_by_country <- merged_data %>%
  group_by(entity) %>%
  summarize(correlation = cor(gdp, life_expectancy))

print(correlation_by_country)
```

```{r}
# Load the world shapefile
world_shapefile <- st_read("D:/github/WG_legacyproject_2024/world-administrative-boundaries/world-administrative-boundaries.shp")

world_shapefile <- world_shapefile %>%
  rename(entity = name)



# Define the lists
list1 <- world_shapefile$entity
list2 <- correlation_by_country$entity

# Find differences
not_in_list1 <- setdiff(list2, list1)
not_in_list2 <- setdiff(list1, list2)

# Print the results
cat("Countries in list2 but not in list1:\n")
print(not_in_list1)

cat("\nCountries in list1 but not in list2:\n")
print(not_in_list2)





# Merge the shapefile with the correlation data
world_shapefile <- left_join(world_shapefile, correlation_by_country, by = "entity")

# Apply the corrections to the correlation data
correlation_by_country$entity <- recode(correlation_by_country$entity, !!!name_correction)




# Plot the correlation on the world map
tm_shape(world_shapefile) +
  tm_polygons("correlation", palette = "RdYlBu", title = "Correlation between GDP and Life Expectancy") +
  tm_layout(title = "Correlation between GDP per Capita and Life Expectancy by Country",
            legend.outside = TRUE)
```

## Analysis in Python

```         
```
